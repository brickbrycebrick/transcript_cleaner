{
  "video_id": "sNa_uiqSlJo",
  "youtube_transcript": "although there's a ton of excitement these days around llms and AI agents I'm more excited about the recent Innovations in text embeddings we saw these in the previous video of this series where we use text embeddings to implement a rag system to improve in llm in this video I'll discuss text embeddings in Greater detail and share two simple yet high value use cases namely text classification and semantic search and if you're new here welcome I'm sha I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the videos that I make there's a fundamental challenge in trying to analyze text put one way text isn't computable meaning we can't do math with text in the same way that we do with numbers for example say you go to a networking event and you're talking with other Professionals in the data space if you wanted to summarize the typical height of every person at the networking event this is something that would be pretty straightforward all you would need to do is measure the heights of everyone at the networking event and then you can summarize these Heights using something like an average and this is something that you can easily compute using a calculator Excel or your favorite programming language however if you have the job descriptions of everyone at this networking event summarizing everyone's roles wouldn't be as straightforward because there's no cell function or mathematical operator that allows you to put in text and generate a summary and so this is where text embeddings come in and put simply text embeddings translate words into numbers so if we have those same job descriptions from the networking event translating these descriptions into text embeddings would look something like this for every single description here we would map it to a set of number numbers but these aren't just any set of numbers these are numbers that capture the inherent meaning of the text in other words text embeddings translate words into meaningful numbers and one way we can see this is take all those numbers from the previous slide and use them to visualize all the job descriptions and so that would look something like this where the numbers that we generated in the text embedding Define the location of each person's job description we can see the data analyst in retail with 5 years of experience is somewhere here and they are located next to a freelance data visualization specialist with four years of experience however these people are relatively far away from this guy who is a data architect with 15 years of experience and so this is the way that text embeddings capture the meaning of the underlying text namely job descriptions that are similar will be located close together while job descrip descriptions that are very different will be located far away from each other from this view the bi analyst in Hospitality early career is very different than the data architect with 15 years of experience so if you've used Chad GPT in the past the thing that might come to mind is sha why should I care about these text embeddings and translating text into numbers can't I just pass all my text to chat GPT and have it figure out what I'm trying to do and the answer is yes there are many tasks that don't require these text embeddings and they're much better suited for chat GPT for example if we wanted to summarize the eight job descriptions from the networking event it would actually be super simple to pass all these to chat GPT and generate a summary however if you have a use case that's not just some oneoff low stakes task but it's something you're trying to integrate into your product or a website or some broader production level Software System then you'd probably want to at least consider using text embeddings before running off and building an AI agent and the reason is if we're talking about AI assistance it's really the early days for this type of technology and there are still a lot of things that we don't understand about using these large language models in this way another downside of using an AI assistant for your use case is that there's a major computational cost associated with running these Large Scale Models another downside is the inherent security risks that come with building llm based applications and so there's a nice list of top 10 llm security risks at reference number three which I recommend you check out for example these are things like prompt injection which are maliciously crafted prompts which get the llm to expose private or confidential data or disrupting some expected decision-making pipeline finally responses from these llm based systems might be unpredictable and prone to hallucinations the system generates fictitious or unhelpful information on the other hand text embeddings have been around for decades and there are several past example applications that we can look to to help guide future applications and use cases another upside of using text embeddings is that they have a much lower computational cost and thus Financial cost than using an entire large language model another is that there are far fewer security risks associated with using embedding model compared to a large language model that users can access through a chat interface finally the responses are more predictable because you know every word or piece of text Will generate a deterministic set of numbers so there's no Randomness in this process now that we have a basic understanding of what text embeddings are and why we should care about them let's see how we can use them for a practical use case the first use case I'm going to talk about is text classification which is the process of assigning a label to a piece of text for example if we were to take all the people from our networking event and their Associated job descriptions a classification task could be trying to determine which people are data analysts and which people are not data analysts based on their respective job descriptions so that might be pretty simple here because anything to the left of this blue line we could label as a data analyst and anything to the right we could label as not a data analyst however text classification is a broadly applicable use case some other situations might be classifying emails as fishing attacks versus not fishing attacks or classifying credit applications as fraudulent or not fraudulent and the list goes on and on and on with this high level understanding let's see what text classification with text embeddings looks like in code so in this example I'm going to take a data set of resumés and try to classify them as either data scientist or not data scientist based on the content of each resume the code in the data set for this example is freely available on the GitHub repository linked here you you to steal this code as a jumping off point for your own use case so the first thing we're going to do is import some helpful libraries here we're going to use open AI text embeddings to use open AI embedding models we'll need to use their their API which requires a secret key if you're unfamiliar with the open aai API or how to get a secret key I walk through that step by step in a previous video of the series next I'll import pandas numai and matte plot lip so we'll use pandas to structure the data we'll use numpy to do math and we'll use map plot lib to visualize things along the way and then finally we will import some things from s learn specifically their random Force classifier which is just a machine learning technique for doing classification and then the ROC Au score which is a way to evaluate a classification model's performance here we're going to import our data from a CSV file and the CSV file is available on the GitHub repo just one note here the data used in this example and the next example were actually synthetically generated using GPT 3.5 turbo although I don't like using synthetic data in these code examples I went with the fake data set here to avoid any privacy issues in using real people's resumés nevertheless this example is instructive in how you can apply text classification to text embeddings next we're going to generate our embeddings so here I'm going to define a function to do this for us basically what this function will do is take the text and my open AI secret key and it'll spit out the text embedding object the way we do that is we set up communication with the open a API and then we just make an API call by passing at the text and specifying which model we want to use this is one of open ai's newer embedding models and it has about 1,500 Dimensions so each piece of text is going to be translated into about 1,500 numbers and then they have another option which has about 3,000 numbers and that's called text embedding 3 large and then the function will return the API call response and then here for every resume in our data set we can generate the embeddings in one line of code then we can extract these text embeddings and store them in a list in this line of code here then we'll store all the text embeddings into a new pandas data frame called DF train so here I'm automatically generating column names so these column names will be embedding underscore 0 embedding underscore 1 embedding underscore 2 all the way up to embedding score like 15 35 or something however many embedding Dimensions there are then we can create the pandas data frame in one line of code so just by passing in this list of text embeddings and the column names and then I'm going to add one more variable to this data frame which is a true false a Boolean variable which indicates whether that row is associated with a data scientist's resume or not a data scientist's resume and then we can split our variables into a set of predictors so X will be all the text embeddings for every one of our resumés y will just be the true false is data sign scientist is not data scientist variable and then we'll pass this to the random Force classifier to train a model so just in one line of code we train our classifier we can print the accuracy and Au value of the model applied to the training data so the output here is one indicating perfect performance of the model on the training data which is super suspicious so let's evaluate the model on a different data set here we're going to load in a testing data set so this is also available on the GitHub repository here we're just repeating the same steps as we did for the training data we'll read it in as a pandas data frame we'll generate the embeddings in one line of code we'll extract the embeddings as a list then we'll create a new pandas data frame for the testing data and then we'll create a Target variable of is data scientist for the test data and then we'll split it into a set of predictors and the target variable with that we can simply evaluate the model on this new data set with the two lines of code and we see the model still has very good performance on the training data however before we kind of celebrate the model is still likely overfitting here and there are two reasons for that first is we have 1,537 predictors but only 100 records in these situations where you have a ton of predictors and very few examples it's not hard for the model to memorize the training data and overfit to it secondly the training data and testing data were generated in identical ways using GP PT 3.5 turbo and so this is one of the downsides of generating synthetic data is that they're going to be artifacts in the data generating process which may create similarities between our training data and testing data that won't be relevant to resumés from The Real World so while there are many ways we can solve the overfitting problem in this situation the best thing would be to one get many more examples of résumés so 100 RS isn't going to cut it probably want close to 1,000 or 10,000 résumés and then second if you want to use this model on real world resumés you should train it on real world resumés moving on to the second use case of semantic search the big idea here is that semantic search returns results based on the meaning of a user's query in contrast to keyword search which looks to match specific words or phrases in the query to a set of search documents or web pages and so the way this works in the context of text embeddings is that say we have a query like I need someone to build my data infrastructure if we were to take everyone from our networking event once again and embed their job descriptions in this concept space here the way we can do semantic search is we can embed the user query into this same space and if we were to do that the query might sit somewhere over here then if we wanted to return search results we could look at the three nearest job descriptions and return them in our search results and for this particular query keyword search may not work so well because there's no mention of the typical role that would be able to build a data infrastructure namely a data engineer however semantic search is able to connect this task of building a data infrastructure to things like data engineer or building data pipelines or ETL process or building a data model and other related skills and activities to building a data infrastructure which may not share the same exact wording here we're going to use text embeddings to implement semantic search in exactly the same way as we saw in the example on the previous slide so we'll use the same data set as in use case one and what we'll do is we'll take a user query such as I need someone to build my data infrastructure and return resumés in our data set which match that user's query again we're going to import some handy libraries we have numpy and pandas and next we import sentence Transformers which is actually an open-source library with many embedding models that you can use completely for free so no need to get an API key or pay to make API calls to open ai's API next again we're going to import things from Psychic learn namely this PCA function function which allows us to do dimensionality reduction and if you're unfamiliar with PCA I actually talked about this in an old video so I'll link that on the screen so you can check it out if you're interested and then we'll import this distance metric function which will allow us to measure the distance between the query and all the different resumés in our database finally we'll import matplot lib to visualize some things okay so here we'll read in the data in the same exact way as before so we'll read in this rumore train.csv file one thing we'll do quickly is that this CSV file has a column called rooll and then there are a handful of different roles but there's one of them that's a sentence it's not a specific role we'll just relabel all those elements as other next we're going to generate the embeddings so the sentence Transformer Library makes this super easy so all we do is import the model and then we can use this do encode method to pass in all the resumés and generate embeddings for all of them and so there are several embedding models to choose from in the sentence Transformers Library here I use this one all mini LM L6 V2 which was specifically designed to do this semantic search task if you're trying to do text classification or some other NLP task I would check out their pre-trained models in their library and try to tailor the model for your specific use case next we can visualize all the different resumés and roles in this embedding space in the following way what's Happening Here here is that I used PCA to reduce the dimensionality of the embedding space from 384 down to 2 so it can fit on this 2D plot here and since this is a lot of code to do the data visualization I don't want to show it here but it's available on the GitHub for anyone who's interested here we can see that even though we're flattening this embedding space from 384 Dimensions down to two we can see that the different roles such as data scientist data engineer machine learn learning engineer AI consultant data entrepreneur and other they tend to be localized pretty well in this visualization so this is a good sign because if we have a query that aligns with a data scientist it will likely be close to a lot of data scientists that can fulfill that role next we can Define our query so we'll use that same I need someone to build out my data infrastructure and then we can incode it in the same exact ways we encoded the resumés using this do encode method that'll generate a one dimension numpy array called query embedding then we can compute the nearest neighbors and so the way we do that is first I'll Define this dist object which is a distance metric and then we can compute the pairwise distances between the query embedding and all the resume embeddings in this embedding array that we defined earlier all these hairwise distances will be stored in discore Array which is a numpy array of the distance between the query and every single resume in the data set and then we can use this ARG sort function from numpy to sort all of the distances in ascending order note that this isn't sorting the disc array it's actually doing two things it's sorting the disc array in ascending order and then returning the index of each of those elements so the first element is not going to be the smallest distance value itself it's going to be the index of the smallest distance value that's all we need to do the semantic search so what that'll look like is if we want to print the roles of the top 10 results we can go back to our original DF resume data frame look at the roll column and then we can just return the first 10 elements in this disc array sorted array that'll look like this so the query I need someone to build out my data infrastructure returns a ton of data engineers which is a good sign because those are the people that will be able to do that another thing we can do is to look at the resume of the top result and so we can do that in this way we go back to our DF res data frame look at the resumé column and then return the resumé that corresponds to the first element in this Idis array sorted array and so the resume looks like this just reading through this highly skilled and experienced data engineer with a strong background in designing implementing and maintaining data pipelines proficient in data modeling ETL processes and data warehousing Adept at working with large data sents and optimizing data workflows to improve efficiency and so notice that nowhere here does it say built data infrastructure but it does say all these things that go into building data infrastructure designing implementing maintaining data pipelines data modeling ETL processes data warehousing scalable data pipelines optimiz ETL processes data architectur there's so many buzzwords and so much jargon in this space and just matching keywords together may not be super helpful but when you use text embeddings which captures the underlying meaning of the text it tends to give good results visualizing these queries in the embedding space it'll look something like this so that same one I need someone to build out my data infrastructure we see that the query is pretty close to all the data Engineers which is a good sign another one is project manager for AI feature development so this one tends to be closer to the machine learning engineers and the AI Consultants and it's kind of like on the border between the two which makes sense because you're going to need an ml engineer for the feature development but you probably want an AI consultant for the project management side of the project and so probably a mix of those two skills will be best and then finally a data engineer with Apache airflow experience pretty close to the data Engineers but it also seems close to this other category which is just randomly generated resumés okay so before celebrating I want to kind of zoom into this particular query here data engineer with Apache airflow experience for this simple semantic search example it seems to struggle with specific search requirements such as I want a data engineer but I specifically want them to know Apachi airflow and so when I pass this into the semantic search system only one of the top five results had airf flow listed on their resume and of the top five it was the third one that had it even though there were three other resumés in the data set that had Apache airflow experience and so this example illustrates that semantic search isn't better than keyword search in all situations each of them has their pros and cons and so if you want to build a robust search system you'll likely want to employ both keyword-based search and semantic search to get the best of both worlds which brings us to a few additional strategies for improving a search system first is hybrid search and so this is exactly what I just mentioned which is bringing together keyword-based search and semantic search while there are many ways we can bring these two approaches together a simple method would be given a user's query apply a keyword based search to it so you'll filter down the search results based on specific words in the query such as data engineer or Apache airflow and then from those results you'd apply the semantic Sur search another option is to use a ranker and so a ranker is a special type of model which takes in a query in a document for example and it spits out a similarity score notice that this is an alternative way to compute the similarity or the difference between two pieces of text common way to use a ranker is let's say you use your semantic search system and you return the top 25 search results from the semantic search you can use the ranker to take those 25 search results and compare them with the query to do an additional ranking and empirically this seems to have pretty good results especially when integrated into a rag system and then finally you can fine-tune an embedding model for a specific domain or use case one of the downsides to these embedding models is that they tend to be trained on a large Corpus of text so while this makes them very very good at general purpose tasks they may fall short in specific domains where there may be heavy use of Jaron for example if I wanted to further improve this semantic search system navigating all the jargon of data engineering ml engineering and data science I could Additionally fine-tune the embeddings on this type of text so if you enjoyed this video and you want to learn more check out the blog published in towards data science and although this is a member only story you can access it completely for free using the friend Link in the description below while this does bring us to the end of the llm series at least for now if there's anything that wasn't included in the series please feel free to drop that in the comment section below and perhaps the series will be resuscitated once again in a few months and as always thank you so much for your time and thanks for watching",
  "cleaned_transcript": "Although there's a ton of excitement these days around LLMs and AI agents, I'm more excited about the recent innovations in text embeddings. We explored these in the previous video of this series, where we used text embeddings to implement a RAG system to improve an LLM. In this video, I'll discuss text embeddings in greater detail and share two simple yet high-value use cases: text classification and semantic search. If you're new here, welcome! I'm Sha, and I make videos about data science and entrepreneurship. If you enjoy this content, please consider subscribing—it's a great, no-cost way to support me and the videos I create.  \n\nThere's a fundamental challenge in trying to analyze text. Put simply, text isn't computable, meaning we can't perform mathematical operations on it in the same way we do with numbers. For example, imagine you're at a networking event talking with other professionals in the data space. If you wanted to summarize the typical height of every person at the event, you'd need a way to quantify that information. This is something that would be pretty straightforward. All you would need to do is measure the heights of everyone at the networking event and then summarize these heights using something like an average. This is something you can easily compute using a calculator, Excel, or your favorite programming language. However, if you have the job descriptions of everyone at this networking event, summarizing everyone's roles wouldn't be as straightforward. There's no built-in function or mathematical operator that allows you to input text and generate a summary. This is where text embeddings come in. Simply put, text embeddings translate words into numbers. So, if we have those same job descriptions from the networking event, translating these descriptions into text embeddings would look something like this: for every single description here, we would map it to a set of numbers. But these aren't just any set of numbers—these are numbers that capture the inherent meaning of the text. Text embeddings translate words into meaningful numerical representations. To illustrate this, we can take the numerical vectors generated from the text embeddings and use them to visualize job descriptions. For example, the numbers from the embeddings define the position of each job description in a semantic space. Here, a data analyst in retail with five years of experience might be located near a freelance data visualization specialist with four years of experience. However, both of these roles are relatively far from a data architect with 15 years of experience. This visualization demonstrates how text embeddings capture the meaning of the underlying text: similar job descriptions are positioned close together, while dissimilar ones are placed farther apart. From this perspective, a business intelligence analyst in hospitality at an early career stage appears quite distinct from the other roles. The data architect with 15 years of experience would be positioned farther away in this semantic space. If you've used ChatGPT in the past, you might wonder: why should I care about text embeddings and translating text into numbers? Can't I just pass all my text to ChatGPT and have it figure out what I need? The answer is yes—there are many tasks that don’t require text embeddings and are better suited for ChatGPT. For example, summarizing the eight job descriptions from the networking event would be straightforward with ChatGPT. However, if your use case isn’t just a one-off, low-stakes task but something you’re integrating into a product, website, or broader production-level software system, you should consider using text embeddings before jumping into building an AI agent. The reason is that when it comes to AI agents, we’re still in the early stages. For this type of technology, there are still many aspects we don’t fully understand when using large language models (LLMs) in this way. Another downside of relying on an AI assistant for your use case is the significant computational cost associated with running these large-scale models. Additionally, there are inherent security risks when building LLM-based applications. A comprehensive list of the top 10 LLM security risks is available at reference number three, which I recommend reviewing. These risks include issues like prompt injection, where maliciously crafted prompts can trick the LLM into exposing private or confidential data or disrupting expected decision-making processes. Furthermore, responses from LLM-based systems can be unpredictable and prone to hallucinations, where the system generates fictitious or unhelpful information. In contrast, text embeddings have been around for decades, and there are numerous past applications we can reference to guide their use effectively. Another advantage of using text embeddings is their significantly lower computational and financial cost compared to deploying a full large language model. Additionally, embedding models pose far fewer security risks than large language models accessed through chat interfaces. Responses from embedding-based systems are also more predictable, as every word or piece of text generates a deterministic set of numbers, eliminating randomness in the process. Now that we understand the basics of text embeddings and their benefits, let’s explore a practical use case. The first use case I’ll discuss is text classification, which involves assigning labels to pieces of text. For example, if we take all the attendees from our networking event and their associated job descriptions, a classification task could involve determining which individuals belong to specific categories. Data analysts can be distinguished from non-data analysts based on their job descriptions. For instance, anything to the left of this blue line could be labeled as a data analyst, while anything to the right could be labeled as not a data analyst. However, text classification is a broadly applicable use case. Other examples include classifying emails as phishing attacks versus non-phishing attacks or identifying fraudulent credit applications. The possibilities are extensive. With this high-level understanding, let’s explore how text classification with text embeddings works in practice. In this example, I’ll use a dataset of resumes to classify them as either data scientist or non-data scientist based on their content. The code and dataset for this example are freely available on the GitHub repository linked here, which you can use as a starting point for your own use case. The first thing we’ll do is import some helpful libraries. Here, we’ll use OpenAI text embeddings. To use OpenAI embedding models, we’ll need to access their API, which requires a secret key. If you’re unfamiliar with the OpenAI API or how to obtain a secret key, I walk through that step-by-step in a previous video in this series. Next, I’ll import Pandas, NumPy, and Matplotlib. We’ll use Pandas to structure the data, NumPy for mathematical operations, and Matplotlib for visualization along the way. Finally, we’ll import some tools from scikit-learn, specifically the Random Forest classifier, which is a machine learning technique for classification, and the ROC AUC score, which is a metric for evaluating a classification model’s performance. Here, we’ll import our data from a CSV file, which is available on the GitHub repository. Just a note: the data used in this example, as well as the next one, were synthetically generated using GPT-3.5. Although I don’t like using synthetic data in these code examples, I opted for a fake dataset here to avoid any privacy concerns related to using real people’s resumes. Nevertheless, this example is instructive in demonstrating how you can apply text classification to text embeddings. Next, we’ll generate our embeddings. Here, I’ll define a function to handle this for us. Essentially, this function will take the text and my OpenAI secret key and return the text embedding object. To do this, we set up communication with the OpenAI API and make an API call by passing the text and specifying which model we want to use. This is one of OpenAI’s newer embedding models, which has about 1,500 dimensions, meaning each piece of text will be translated into approximately 1,500 numbers. There’s also another option with about 3,000 dimensions, called `text-embedding-3-large`. The function will then return the API call response. For every... For each resume in our dataset, we can generate embeddings in one line of code. Then, we extract these text embeddings and store them in a list, as shown in this line of code. Next, we’ll store all the text embeddings into a new Pandas DataFrame called `df_train`. Here, I’m automatically generating column names, such as `embedding_0`, `embedding_1`, `embedding_2`, and so on, up to `embedding_1535` or however many embedding dimensions there are. We can create the Pandas DataFrame in one line of code by passing in the list of text embeddings and the column names. Then, I’ll add one more variable to this DataFrame: a Boolean variable indicating whether each row corresponds to a data scientist’s resume or not. Finally, we can split our variables into a set of predictors: `X` will contain all the text embeddings for each resume, and `y` will be the Boolean labels indicating whether the resume belongs to a data scientist. The Boolean variable indicates whether a resume is for a data scientist or not. We’ll then pass this to a random forest classifier to train a model. In just one line of code, we train our classifier and print the accuracy and AUC value of the model applied to the training data. The output here is 1, indicating perfect performance on the training data, which is highly suspicious. To validate the model, let’s evaluate it on a different dataset. We’ll load a testing dataset, which is also available on the GitHub repository. We’ll repeat the same steps as we did for the training data: read it in as a Pandas DataFrame, generate the embeddings in one line of code, extract the embeddings as a list, and create a new Pandas DataFrame for the testing data. We’ll then create a target variable, `is_data_scientist`, for the test data and split it into a set of predictors and the target variable. With that, we can evaluate the model’s performance. On this new dataset, with just two lines of code, we observe that the model still performs very well on the training data. However, before celebrating, it’s important to note that the model is likely overfitting. There are two main reasons for this: first, we have 1,537 predictors but only 100 records. In situations where there are many predictors and very few examples, it’s easy for the model to memorize the training data and overfit. Second, both the training and testing data were generated in identical ways using GPT-3.5 Turbo. This is one of the downsides of synthetic data generation—artifacts in the data-generating process can create similarities between the training and testing data that may not reflect real-world resumes. While there are many ways to address overfitting in this scenario, the best approach would be to obtain significantly more resume examples. A dataset of just 100 resumes is insufficient; we’d likely need a much larger and more diverse dataset. To address overfitting, we would ideally need close to 1,000 or 10,000 real-world resumes. Additionally, if you intend to use this model on real-world resumes, it should be trained on real-world data. Moving on to the second use case—semantic search—the core idea is that semantic search retrieves results based on the meaning of a user's query, unlike keyword search, which matches specific words or phrases in the query to documents or web pages. In the context of text embeddings, here's how it works: Suppose we have a query like, \"I need someone to build my data infrastructure.\" If we embed the job descriptions of attendees from our networking event into a semantic space, we can also embed the user query into the same space. The query might then reside in a specific location within this space. To return search results, we could identify the three nearest job descriptions to the query and present them as the search results. For this particular scenario... Query keyword search may not work well in this case because there's no mention of the typical role that would be responsible for building a data infrastructure, such as a data engineer. However, semantic search can connect the task of building a data infrastructure to related roles like data engineer, as well as activities such as building data pipelines, ETL processes, or data models—even if these terms don’t share the exact same wording. Here, we’ll use text embeddings to implement semantic search in the same way as demonstrated in the previous example. We’ll use the same dataset as in Use Case 1, where we’ll take a user query like, \"I need someone to build my data infrastructure,\" and return resumes from our dataset that match the query. To do this, we’ll import some useful libraries, including NumPy and Pandas, followed by Sentence Transformers, an open-source library that provides a variety of embedding models. You can use this completely for free, so there’s no need to obtain an API key or pay for API calls to OpenAI’s API. Next, we’ll import tools from scikit-learn, specifically the PCA function, which allows us to perform dimensionality reduction. If you’re unfamiliar with PCA, I’ve covered it in a previous video, and I’ll link it on the screen for reference. We’ll also import a distance metric function to measure the distance between the query and all the resumes in our database. Finally, we’ll import Matplotlib to visualize the results. \n\nHere, we’ll read in the data in the same way as before, using the `resume_train.csv` file. One quick adjustment we’ll make is to the column labeled \"role.\" While most entries specify a role, some contain sentences instead of specific roles. We’ll relabel those entries as \"other.\" Next, we’ll generate... the embeddings. The Sentence Transformers library makes this process straightforward. All we need to do is import the model and use the `encode` method to pass in all the resumes and generate embeddings for them. There are several embedding models available in the Sentence Transformers library. Here, I use the `all-MiniLM-L6-v2` model, which is specifically designed for semantic search tasks. If you're working on text classification or other NLP tasks, I recommend exploring their pre-trained models and selecting one tailored to your specific use case. Next, we can visualize the resumes and roles in this embedding space. To do this, I used PCA to reduce the dimensionality of the embedding space from 384 dimensions down to 2, allowing it to fit on a 2D plot. Since the code for data visualization is extensive, I won’t display it here, but it’s available on GitHub for anyone interested. Here, we can see that even though we're flattening the embedding space from 384 dimensions down to two, the different roles—such as data scientist, data engineer, machine learning engineer, AI consultant, data entrepreneur, and others—tend to cluster well in this visualization. This is a good sign because if we have a query that aligns with a data scientist, it will likely be close to many data scientists who can fulfill that role. Next, we can define our query. We'll use the same example: \"I need someone to build out my data infrastructure.\" We'll encode it in the same way we encoded the resumes, using the `encode` method, which will generate a one-dimensional NumPy array called `query_embedding`. Then, we can compute the nearest neighbors. To do this, I'll first define a `dist` object, which is a distance metric, and then compute the pairwise distances between the `query_embedding` and all the resume embeddings in this embedding space. The pairwise distances will be stored in a `dist_array`, which is a NumPy array representing the distance between the query and every resume in the dataset. We can then use the `argsort` function from NumPy to sort these distances in ascending order. Note that this function doesn't sort the `dist_array` directly; instead, it sorts the array and returns the indices of the elements in ascending order. This means the first element in the sorted array isn't the smallest distance value itself but the index of the smallest distance value. This is all we need to perform the semantic search. To print the roles of the top 10 results, we can reference the original `resume_df` DataFrame, look at the `role` column, and return the first 10 elements from the sorted `dist_array`. For example, the query \"I need someone to build out my data infrastructure\" returns a list of roles, primarily data engineers, as expected. The results primarily include data engineers, which is a good sign because these are the individuals best suited for the task. Another thing we can do is examine the resume of the top result. To do this, we return to our `resume_df` DataFrame, look at the `resume` column, and retrieve the resume corresponding to the first element in the sorted `dist_array`. The resume reads as follows: \"Highly skilled and experienced data engineer with a strong background in designing, implementing, and maintaining data pipelines. Proficient in data modeling, ETL processes, and data warehousing. Adept at working with large datasets and optimizing data workflows to improve efficiency.\" Notice that while the resume doesn't explicitly mention \"building data infrastructure,\" it includes all the relevant skills, such as designing, implementing, and maintaining data pipelines, data modeling, ETL processes, data warehousing, scalable data pipelines, and optimizing ETL processes and data architecture. There are so many buzzwords and jargon in this space, and simply matching keywords may not be particularly effective. However, when you use text embeddings, which capture the underlying meaning of the text, the results tend to be much more accurate. Visualizing these queries in the embedding space, it looks something like this: for the query \"I need someone to build out my data infrastructure,\" we see that the query is positioned close to all the data engineers, which is a good sign. Another example is \"project manager for AI feature development,\" which tends to be closer to the machine learning engineers and AI consultants. It sits on the border between the two, which makes sense because you’ll likely need an ML engineer for the feature development and an AI consultant for the project management side. A mix of these two skill sets would likely be ideal. Finally, for the query \"data engineer with Apache Airflow experience,\" the result is also close to the data engineers, as expected. It also seems close to another category, which is just randomly generated resumes. Before celebrating, I want to zoom in on this particular query: \"data engineer with Apache Airflow experience.\" In this simple semantic search example, the system seems to struggle with highly specific search requirements, such as finding a data engineer who specifically knows Apache Airflow. When I pass this query into the semantic search system, only one of the top five results had \"Airflow\" listed on their resume, and it was the third result. Interestingly, there were three other resumes in the dataset that had Apache Airflow experience, but they didn’t rank higher. This example illustrates that semantic search isn’t always better than keyword search—each has its pros and cons. To build a robust search system, you’ll likely want to combine both keyword-based and semantic search to get the best of both worlds. This brings us to a few additional strategies... strategies for improving a search system. First is hybrid search, which is exactly what I just mentioned—combining keyword-based search and semantic search. While there are many ways to integrate these two approaches, a simple method would be to apply a keyword-based search to a user's query, filtering down the results based on specific words in the query, such as \"data engineer\" or \"Apache Airflow.\" Then, from those filtered results, you’d apply semantic search. Another option is to use a ranker, which is a special type of model that takes in a query and a document, for example, and outputs a similarity score. This is an alternative way to compute the similarity or difference between two pieces of text. A common way to use a ranker is to first retrieve the top 25 search results from your semantic search system and then use the ranker to compare those 25 results with the query. The query can undergo additional ranking, and empirically, this approach yields strong results, especially when integrated into a RAG system. Finally, you can fine-tune an embedding model for a specific domain or use case. One downside of these embedding models is that they are typically trained on a large corpus of text, making them excellent for general-purpose tasks but potentially less effective in specialized domains with heavy jargon. For instance, to further improve this semantic search system for navigating the jargon of data engineering, ML engineering, and data science, you could fine-tune the embeddings on domain-specific text. If you enjoyed this video and want to learn more, check out the blog published on Towards Data Science. Although it’s a member-only story, you can access it for free using the friend link in the description below. This brings us to the end of the LLM series—at least for now. If there's anything that wasn't covered in the series, feel free to share your thoughts in the comments below. Perhaps the series will be revived in a few months. As always, thank you so much for your time and for watching!",
  "success": true,
  "total_tokens": 38903
}