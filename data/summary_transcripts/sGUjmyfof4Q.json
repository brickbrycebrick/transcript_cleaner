{
  "video_id": "sGUjmyfof4Q",
  "youtube_transcript": "this is lanch blank chain deep seek just released R1 which is a new fully open source reasoning model from the deep seek lab and it comes with a paper that describes their training strategy which is quite cool because reasoning models represent a new scaling Paradigm for llms I have a separate video on this that's also coming out uh soon that you can check out so scaling Paradigm over the past few years has been next token prediction we've seen many successful chat models trained with this it's system one type thinking it's fast intuitive we often tell the model how to think with tricks like how to think you know think step by step and the interaction modes of in chat now reasoning models are a bit different they're trained with a different Paradigm RL on chain of thoughts we'll talk about that a lot in a bit it's system to reasoning often tell the model what you actually want not how to think and the interaction mode is a bit different it's very good for research or planning in the background style tasks that are less interactive so the really interesting thing here is that we now know how a state-of-the-art reasoning model is trained so of course the current stateoftheart reasoning models from open AI the O Series models are closed Source we don't have detailed information about how training works but this is a very clear illustration of how they built a state-of-the-art reasoning model that is on par with 01 and you'll see those results here shortly but let me actually talk through the training strategy it's very interesting so deepcar one uses a combination of fine-tuning and reinforcement learning to produce this reasoning model and has a few different stages so the first stage is just fine tuning they take deep seek V3 which is their very strong base chat model and they fine-tune it on some number of thousands of Chain of Thought reasoning examples now they don't actually tell the specific number of examples from my reading of the paper but the point is they do a fine-tuning phase to build a good starting point for RL now the second stage here is reinforcement rolling with this approach grpo so what's going on there well they have a separate paper on this and I do want to talk about that a little bit so this RL stage one uses grpo reinforcement learning it's from this deep seek math paper now here's what's going on for every training example now I do want to make a note how many training examples they they use they have 144,000 training examples of hard verifiable problems in math and coding for which you need some degree of reasoning like you need to produce a reasoning Trace typically to solve them and there's some definitive solution that can be verified those are the two criteria that matter here so they have all these samples now what they're doing is for every training example they actually produce 64 samples or 64 different attempts to solve the problem and they score each one of those with some rule-based reward like correct or incorrect for math or coding right that's pretty straightforward now here's where it's kind of interesting they basically compare every sample to the mean of all samples in this 64 sample batch okay so that's kind of what they do for samples with high or low normalize reward relative to the group meme they increase or drop the probability of the model generating all the tokens in that sequence so what happens is each token in that output gets a positive or negative gradient update and here's the intuition it's basically saying let's make all the choices that led to this correct or incorrect decision more or less likely so that's actually what they're doing and so the punchline here is why do they do this they're trying to discover good reasoning patterns and what happens is this makes the model very strong at reasoning but it loses some general capacities for example they mentioned it had potential language mixing issues so this is where another interesting trick comes into play if we look at our diagram here we're right here so we've actually use reinforcement learning to get a very strong reasoning model but it's actually a bit weaker than some other capabilities so what they do is they take the resulting reasoning traces from that model and filter them to get only high quality ones so the paper talks about this as rejection sampling so they're basically filtering the outputs of that first reinforcement learning phase for a bunch of different things it's not just correctness but the point is it results in 600,000 reasoning traces that they can then train further on so that's really the interesting Insight here that you can utilize outputs from the first stage of RL to subsequent stages of your model and that's exactly what they do so they do a second stage of fine-tuning on the results of that that sampling plus 200,000 additional non- reasoning samples in writing and factual QA to what they describe as restore the general model capabilities while baking in high quality reasoning so if you look at the diagram here what's happening is they are filtering the outputs of that first phase of RL okay so remember this model is a very strong Reasoner but it's weak in some of General capabilities and they're combining some non- reasoning examples in there for writing in QA and they're fine-tuning on all that and then what they argue is or they present they get a model that retains very strong reasoning but also restores General capabilities okay so that's the key point then after that they have a final or second round of reinforcement learning with two different rewards so previously we talked about they only used a rulebase reward for reasoning on math and coding style problems now they include different rewards for helpfulness and harm as well as reasoning and they use a mix of data that includes both reasoning and general problems to really optimizing for both reasoning and general capabilities okay so that's really the second stage of RL now a final note which actually is very exciting and we're going to be actually working with more directly here they also take that data set that they get from that first phase of RL of 600,000 samples and they actually do knowledge so they take much smaller open source models and fine-tune them on those high quality reasoning traces and then what they get is a bunch of distilled smaller R1 models pretty cool and some of them you can actually run your laptop as we'll see right here so what are the results so they show a bunch of nice results here deeps car1 versus 01 and some smaller models ow and mini really the punchline is it's very close to R1 on a bunch of interesting challenges related to coding and math now one in particular pay attention to S bench verified it's a very popular Benchmark for General software engineering challenges and it is doing indeed quite well slightly better than 01 apparently they also have a bunch of distilled models so here's the thing that is really quite cool if you look at their distilled quen 14b you look at the Benchmark results and it is pretty close to 01 mini you can kind of go across and look and convince yourself of this but look it's pretty strong and 14b can actually run on a lot of people's laptops for example I have a 32 gig MacBook Pro and and I can run 14b as we'll see in a bit so now let's try playing with it so I pulled deep seek 14b from AMA so you can see they put a tweet out recently they host all these models it's pretty cool you can try to run them on your own hardware and I'm in a notebook so all I need to do is grab L chain of llama I'm going to initialize my model and what's nice is I'm going to use Json mode with AMA to also produce Json outputs and see how well structured outputs work with this this model so first let's try a simple question was the capital of France Cool C you see the capital of France at Paris we see something else is interesting these think tokens okay if you go around local Lama and you you hunt around for this there's a lot of people talking about these think tokens they're hard to prompt away I've tried a bunch they've seem to be kind of an annoying thing they are absolutely part of the training process you can look at the paper you can see that these think tokens are actually included in the training now let's try Json mode so what's interesting is when you use Json mode the think tokens are not present so there is some postprocessing happening on the Alama side that strips them and you get ajacent object out so looks like Json mode Le is working so that's a good thing I ask a more involved question Give me a summary on scaling laws for RL models and again you see wow this is quite verbose and you can see this think token emitted first just like before and now you get like a much more detailed breakdown of uh its internal thought process so let's go overhead and look at Langs Smith just to kind of get a better view of this output so in my Langs Smith project now I'm going to open up this Trace we can see it took 64 seconds okay so that's actually pretty long but again I'm really pushing the limits of my Hardware running the 14b model so that's fine it's a little bit on me um but I want to test 01 mini level performance running locally based on the Benchmark so I just want to kind of play with it okay so here's the output again it's quite verbose you can see the think token here emitted so it does a lot of thinking prior to responding and seems to provide a sane response but again this issue of a lot of pre-thinking being emitted is evidently an issue with these models uh that may be a problem not depending on your application you can also programmatically try to remove that that's another thing to think about so let's Vibe test a bit more this is a repo called olama de researcher and this is basically an evaluator Optimizer workflow for report writing so I'm going to do is I'm going to have an open source llm running locally via Ama take an input topic from a user and generate a search query for it perform web search get the results produce a summary but then reflect on the summary and regenerate a question go back so this Loop is going to look kind of like this you can see queer generation research summary generation reflection new query and so forth this will continue for some set number of Cycles that's configurable and in the end I'll get a nice summary with sources and this could be r with any open source llm now I have a separate video on this that talks about building from scratch so I'm not going to kind of build everything again but I will just test this out using R1 so just some specifics here I have a MacBook Pro M2 Max 32 gig I found that the 14 billion distilled deeps R1 model is about at the edge of what I can run but still it's fun to try as discussed before all I need to do to run this is basically just set my Tav API key that allows for web search and kick off this command when you do that you're going to see langra Server spin up in your browser and you can actually start interacting with it directly so you're going to see this in your browser so this is pretty nice this a little environment that I like to use to play with assistants that I create using Lang graph you can see this shows the overall flow of our assistant here so it's going to generate a query do web search summarize the results reflect and go back this is a nice test bed though for looking at different local models so what you can do is open up this configurable thing just paste whatever local model on a llama that you've downloaded and want to test so in my case let's test 14b I'll have it iterate twice so go two loops and we can ask any question here so let's say give me a report on RL training approaches so fine all I have to do is submit so it's Jing our query and it's using structured output that's good so that part of the flow is working as expected nice now it's using tavali to do web research you can look at the repo to dig into that I have a separate video on all this in detail so now it's summarizing my sources so this is kind of nice you see it stream as it goes you can see those think tokens again now in the repo I added a filter to remove them because I found that they do affect some Downstream processing so I'm going to filter this out when I save it to state in My Graph but you can see how much reasoning and thinking it's doing so now it's reflecting on my summary and I've stripped out those thinking tokens during this reflection phase so this is pretty cool it finds a knowledge Gap generates a followup question and now I've done more web research and I'm summarizing based upon my initial summary and the new web reg resources that I've retrieved so now here's my updated summary so it's thinking it needs to extend the summary okay I mean you know that may not be a bad thing that it is expressing its kind of process for us to see I mean look it's listening to the promer to reason need to seamlessly integrate these points good um it's highlighting the new resource that it found good okay so it's being very expressive about everything it needs to do based on the instructions I give it it's done thinking and cool it is updating that summary so you can see now reinforc learnings a sub field of AI it's going to reflect again go back it'll try that one more time and again it pulled a new paper it's thinking I need to extend the summary further I mean in a way I kind of like this thinking process cuz it really explains what it's actually doing and how it's freezing about it and you can see it's updating its summary and it actually looks pretty sane pretty nice and it gives us information about the global market for RL so fair enough and an exit so now we get the final summary so my assistant will add this little summary thing to the top and we have our nice written summary here and it'll add the sources and we can go ahead and look at that in Langs Smith as well we can see everything it did we can look at the final summary which is actually right here so here's the summary and there's our sources so basically my take is the think tag thing is kind of annoying I actually kind of like to see that as a developer it's annoying to manage if you're actually trying to build an application with this because it emits that to the output you have to process it out the 14 billion parameter model is at the edge of what I can run locally on my laptop of course it depends on your hardware and the summary looks quite nice and comprehensive and all run locally so it's all for free so listen it's pretty cool that you can have these reasoning models now running locally on your laptop they'll obviously will get better this think token issue will be resolved I'm sure in the near future I encourage you to play with it very much I find AMA to be a very nice easy way to access these models but there's also some other ways to do it and I think it's a really nice step forward and it is really cool that we actually have visibility to how these models are trained and that this is all open source so thank you to deep seek for releasing this and for llama forg getting this up really quickly so anyway hope this was informative and um thanks",
  "cleaned_transcript": "DeepSeek Lab has just released R1, a new fully open-source reasoning model, accompanied by a paper detailing its training strategy. This is particularly exciting because reasoning models represent a new scaling paradigm for LLMs. I have a separate video on this topic coming out soon, which you can check out. Over the past few years, the dominant scaling paradigm has been next-token prediction, which has led to many successful chat models. This approach aligns with System 1 thinking—fast and intuitive. We often guide the model on how to think using techniques like \"think step by step,\" which is common in chat interactions. Reasoning models, however, operate differently. They are trained using a different paradigm, such as reinforcement learning (RL) on chain-of-thought reasoning. We'll delve into this in more detail shortly. This approach aligns with System 2 reasoning, where the focus is on specifying what you want rather than how to think. The interaction mode also differs, making these models particularly effective for research or planning tasks. Background-style tasks that are less interactive are particularly well-suited for reasoning models. The really interesting thing here is that we now have a clear understanding of how a state-of-the-art reasoning model is trained. Unlike OpenAI's O-series models, which are closed-source and lack detailed information about their training processes, DeepSeek-R1 provides a transparent illustration of how to build a reasoning model that is on par with OpenAI's models. You'll see those results shortly, but let me walk you through the training strategy, which is quite fascinating. DeepSeek-R1 uses a combination of fine-tuning and reinforcement learning to produce this reasoning model, and the process involves several stages. The first stage is fine-tuning: they take DeepSeek-V3, their highly capable base chat model, and fine-tune it on thousands of chain-of-thought reasoning examples. While the exact number of examples isn't specified in the paper, the key takeaway is clear. They begin with a fine-tuning phase to establish a strong foundation for reinforcement learning. The second stage involves reinforcement learning using the GRPO approach. This RL stage is detailed in a separate DeepSeek math paper, which I’ll touch on briefly. Here’s how it works: for each training example—and it’s worth noting they use 144,000 hard, verifiable problems in math and coding—they generate 64 different solution attempts. These problems require reasoning traces to solve and have definitive, verifiable solutions, which are the key criteria. Each of the 64 attempts is scored using a rule-based reward system, primarily based on correctness. Now, here's where it gets interesting: they compare each sample to the mean of all samples in the 64-sample batch. For samples with high or low normalized rewards relative to the group mean, they adjust the probability of the model generating all the tokens in that sequence. Specifically, each token in the output receives a positive or negative gradient update. The intuition here is to make the choices that led to a correct or incorrect decision more or less likely. Essentially, they're trying to discover and reinforce good reasoning patterns. However, this approach, while making the model very strong at reasoning, can cause it to lose some general capabilities. For instance, they noted potential issues like language mixing. This is where another interesting technique comes into play. We’re now looking at the diagram, and we’ve reached the point where reinforcement learning has been used to create a very strong reasoning model. However, this model is slightly weaker in some other capabilities. To address this, they take the reasoning traces generated by the model and filter them to retain only high-quality ones. This process, referred to as rejection sampling in the paper, involves filtering the outputs from the first reinforcement learning phase based on multiple criteria, not just correctness. The result is a refined set of 600,000 reasoning traces that can be used for further training. This is a key insight: leveraging outputs from the initial RL stage to inform subsequent stages of the model. They then perform a second stage of fine-tuning using these filtered traces, along with 200,000 additional non-reasoning samples in writing and factual QA. This step is designed to restore the model’s general capabilities while integrating the specialized reasoning skills. High-quality reasoning is achieved by filtering the outputs from the first phase of reinforcement learning (RL). As we’ve discussed, this model excels at reasoning but is weaker in some general capabilities. To address this, they combine non-reasoning examples, such as writing and factual QA, and fine-tune the model on this mixed dataset. The result is a model that retains strong reasoning abilities while restoring its general capabilities. This is a critical step. Following this, they conduct a second round of reinforcement learning, this time incorporating multiple rewards. Previously, they used a rule-based reward focused on math and coding problems. Now, they introduce additional rewards for helpfulness and harmlessness, alongside reasoning. They also use a mixed dataset that includes both reasoning and general tasks, optimizing the model for both specialized reasoning and broader capabilities. This marks the second stage of RL. A final note, which is actually very exciting and something we’ll explore more directly here: they also take the dataset from the first phase of RL, consisting of 600,000 high-quality reasoning traces, and perform knowledge distillation. They fine-tune much smaller open-source models on these traces, resulting in a set of distilled, smaller R1 models. This is pretty cool—some of these distilled models can even run on your laptop, as we’ll see shortly. So, what are the results? They show a series of impressive benchmarks here, comparing DeepSeek-R1 against O1 and some smaller models like OW and Mini. The key takeaway is that R1 performs very closely to these models on a range of challenging coding and math tasks. One benchmark to pay attention to is SWE-bench, a popular benchmark for general software engineering challenges, where R1 performs slightly better than O1. Additionally, they have a variety of distilled models, which is quite remarkable. Cool. If you look at their distilled 14B model, the benchmark results show it’s pretty close to O1-mini. You can go through the results and verify this yourself—it’s quite strong. And the 14B model can actually run on many people’s laptops. For example, I have a 32GB MacBook Pro, and I can run the 14B model, as we’ll see shortly. Now, let’s try playing with it. I pulled DeepSeek-14B from Ollama—you can see they recently tweeted about it. They host all these models, which is pretty cool, and you can try running them on your own hardware. I’m in a notebook, so all I need to do is grab LangChain and Llama. I’ll initialize my model, and what’s nice is I’ll use JSON mode with Ollama to produce structured JSON outputs. Let’s see how well this model handles structured outputs. First, let’s try a simple question: What is the capital of France? Cool, it correctly identifies Paris as the capital of France. But something else is interesting—these \"think tokens.\" If you’ve worked with local Llama models, you might have noticed... You’ll notice that many people are talking about these \"think tokens,\" which can be tricky to prompt away. I’ve tried various approaches, but they seem to be a persistent and somewhat annoying artifact of the training process. If you look at the paper, you’ll see that these think tokens are intentionally included in the training. Now, let’s try JSON mode. What’s interesting is that when you use JSON mode, the think tokens are not present. This suggests there’s some post-processing happening on the Ollama side that strips them out, resulting in a clean JSON object. It looks like JSON mode is working well, which is a good thing. Next, I asked a more involved question: \"Give me a summary on scaling laws for RL models.\" Again, the output is quite verbose, and you can see the think token emitted first, just like before. This time, you get a much more detailed breakdown of its internal thought process. To get a better view of this output, let’s switch over to LangSmith. In my LangSmith project now... I'm going to open up this trace. We can see it took 64 seconds, which is actually pretty long, but again, I'm really pushing the limits of my hardware running the 14B model. That's fine—it's a little bit on me. I want to test O1-mini-level performance running locally based on the benchmarks, so I just want to play with it a bit. Here's the output again; it's quite verbose. You can see the think token emitted here, so it does a lot of thinking prior to responding and seems to provide a sane response. But again, this issue of a lot of pre-thinking being emitted is evidently an issue with these models. That may or may not be a problem depending on your application. You can also programmatically try to remove it, which is another thing to consider. Let's vibe test a bit more. This is a repo called Ollama Researcher, and it's basically an evaluator-optimizer workflow for report writing. What I'm going to do is have an open-source LLM running locally via Ollama take an... The user inputs a topic, and the system generates a search query, performs a web search, retrieves the results, and produces a summary. It then reflects on the summary, regenerates a question, and loops back. This cycle—query generation, research, summary generation, reflection, and new query—continues for a configurable number of cycles. In the end, you get a well-structured summary with sources. This workflow can be run with any open-source LLM. I have a separate video discussing how to build this from scratch, so I won’t go into that again here. Instead, I’ll test it using R1. For context, I’m using a MacBook Pro M2 Max with 32GB of RAM. I’ve found that the 14-billion-parameter distilled DeepSeek-R1 model is about the limit of what I can run locally, but it’s still fun to experiment with. As mentioned earlier, all I need to do is set my Tav API key to enable web search and kick off the command. When you do that, you’ll... To see Langra Server spin up in your browser, you can start interacting with it directly. This is a nice environment I like to use to play with assistants I create using LangGraph. You can see the overall flow of our assistant here: it generates a query, performs a web search, summarizes the results, reflects, and loops back. This is a great test bed for evaluating different local models. You can open up this configurable interface, paste whatever local model on Ollama you've downloaded, and test it. In my case, I'll test the 14B model, have it iterate twice (two loops), and ask a question like, \"Give me a report on RL training approaches.\" All I have to do is submit, and it's generating our query using structured output, which is good—that part of the flow is working as expected. Now, it's using Tavily to do web research, and you can observe the process. The repo to dig into—I have a separate video covering all this in detail—is now summarizing my sources. It’s nice to see it stream as it goes, and you can observe those think tokens again. In the repo, I added a filter to remove them because I found they can affect some downstream processing. I’ll filter them out when saving to state in my graph, but you can see how much reasoning and thinking it’s doing. Now, it’s reflecting on my summary, and I’ve stripped out those think tokens during this reflection phase. This is pretty cool—it identifies a knowledge gap, generates a follow-up question, and performs additional web research. It’s now summarizing based on the initial summary and the new web resources retrieved. Here’s the updated summary. It’s thinking it needs to extend the summary, which may not be a bad thing—it’s expressing its reasoning process for us to see. Look, it’s listening to the prompt to reason. It needs to seamlessly integrate these points. It's highlighting the new resource it found, which is good. It's being very expressive about everything it needs to do based on the instructions I gave it. It's done thinking now, and it's updating the summary. You can see it's reflecting again, going back, and trying one more time. It pulled a new paper and is thinking it needs to extend the summary further. In a way, I kind of like this thinking process because it really explains what it's doing and how it's reasoning through it. You can see it updating its summary, and it actually looks pretty sane and nice. It gives us information about the global market for reinforcement learning, which is fair enough. Now, we get the final summary. My assistant will add this little summary to the top, and we have our nicely written summary here. It'll also add the sources, and we can go ahead and look at that in LangSmith as well. You can see everything it did, and we can look at the final summary, which is right here. Here's the summary, and there are our sources. My take is that the think token thing is kind of annoying, but as a developer, I actually like seeing it. It’s annoying to manage if you're trying to build an application with this because it emits those tokens to the output, and you have to process them out. The 14 billion parameter model is at the edge of what I can run locally on my laptop, though of course, it depends on your hardware. The summary looks quite nice and comprehensive, and it’s all run locally, so it’s all for free. It’s pretty cool that you can have these reasoning models running locally on your laptop now. They’ll obviously get better, and I’m sure the think token issue will be resolved in the near future. I encourage you to play with it—I find Ollama to be a very nice, easy way to access these models, but there are also other ways to do it. I think it’s a really nice step forward. Moving forward, it’s really exciting that we now have visibility into how these models are trained, and that this is all open-source. A big thank you to DeepSeek for releasing this and to the Ollama team for getting it up and running so quickly. I hope this was informative, and thanks for following along!",
  "success": true,
  "total_tokens": 27018
}